# ğŸ§  Deep Learning Model Training & Visualization  

## ğŸš€ Project Overview  
This project implements and visualizes multiple deep learning models using **TensorFlow/Keras**. It includes:  
âœ”ï¸ Data preprocessing & exploration  
âœ”ï¸ Building & training different models (Basic NN, CNN, ViT, and a combined CNN+ViT model)  
âœ”ï¸ Saving models & visualizing architectures  
âœ”ï¸ Evaluating and summarizing model performance  

---

## ğŸ›  Dependencies  
Ensure you have the following Python libraries installed before running the notebook:  
```bash
pip install tensorflow matplotlib seaborn numpy pandas pydot graphviz
```

---

## ğŸ’Ÿ How to Run  
1ï¸âƒ£ Open the Jupyter Notebook (`Sessional.ipynb`) in **Jupyter Lab or Colab**.  
2ï¸âƒ£ Run each cell sequentially to train and evaluate the models.  
3ï¸âƒ£ Model architecture plots will be **saved and displayed**.  
4ï¸âƒ£ Final trained models are **saved in `.h5` format** for further use.  

---

## ğŸ“Š Model Performance Summary  
The table below summarizes the accuracy of each model:  

| ğŸ† Model        | ğŸ¯ Accuracy |
|---------------|------------|
| ğŸ§¬ Basic NN   | **87.67%** |
| ğŸ–¼ï¸ CNN        | **92.88%** |
| ğŸŒ€ ViT        | **85.39%** |
| ğŸ”— CNN+ViT   | **94.3%** |

_(These values are placeholders; update them after training your models.)_  

---

## ğŸ” Data Preprocessing  
âœ”ï¸ Checked for **missing values**  
âœ”ï¸ Normalized **pixel values to [0,1]**  
âœ”ï¸ **One-hot encoded** the labels  
âœ”ï¸ Explored **data distribution**  

---

## ğŸ¢ Model Architectures  

### ğŸ§¬ 1. Basic Neural Network (NN)  
- A simple **fully connected feedforward network**  
- Uses **dense layers** with ReLU activation  
- Suitable for **basic classification tasks**  
- Architecture is saved as `basic_model_architecture.png`  

### ğŸ–¼ï¸ 2. Convolutional Neural Network (CNN)  
- Extracts **spatial features** from images  
- Uses **Conv2D, MaxPooling, Flatten, and Dense layers**  
- Architecture is saved as `cnn_model_architecture.png`  

### ğŸŒ€ 3. Vision Transformer (ViT)  
- Uses **self-attention mechanism** for feature extraction  
- Processes images as **patch embeddings**  
- Powerful for **complex pattern recognition**  
- Architecture is saved as `vit_model_architecture.png`  

### ğŸ”— 4. Combined CNN + ViT  
- **Hybrid model combining CNN feature extraction with ViT attention**  
- Achieves **higher accuracy** than individual models  
- Architecture is saved as `combined_model_architecture.png`  

---

## ğŸ“‰ Future Improvements  
ğŸš€ Hyperparameter tuning for **better accuracy**  
ğŸ“ˆ Experimenting with **larger datasets**  
ğŸ” Trying advanced models like **ResNet, EfficientNet**  

---

## ğŸ‘¨â€ğŸ’» Author  
ğŸŒ Location: Nepal ğŸ‡³ğŸ‡µ  

---

ğŸŒŸ **If you found this project useful, give it a star!** ğŸŒŸ  

